{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Ensembling and Stacked Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay everyone, I'm sure this isn't news to many of you but I mostly wrote this for myself. I thought it was fun and maybe it is informative for you or maybe I'm out to lunch :) or and you can make it better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of ensembling is simple. Take a bunch of predicted outcomes from a model(s) and put them together in some way so that they make a better prediction. There are several different approaches such as majority voting, averaging, and weighted averaging. So, can you take 3 predicted results and get a better result simply by averaging them out? Sure you can, but the more predictions you make the better the ensemble will be. \n",
    "\n",
    "What other ways can I improve my ensemble results? While more model runs helps, running different models can also provide different information. So if you are running a CNN - why not run 3 different ones? Use your grid search results to give you a set of predictions you can ensemble. Run multiple XGBoost or random forest models using different hyperparameters. \n",
    "That makes sense - run more models. What else can I do? Run different kinds of models! Run RF and XGB and ETs, and sombine that with results from other techniques - k-NN? Sure! A ConvNet? Why not!\n",
    "\n",
    "Is there anything else I can do? Different dataset or different k-folding values can give you different predictions. Add more data or leave some out. Try pseudo-labelling or augmentation if you can to add data for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great- I can ensemble like the best of them now!\n",
    "## So - what is stacked generalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why stacked generalization then if ensembling is so good already? \n",
    "\n",
    "Well, think about it. What is one set of predictions ends up with big outliers and you are doing only 5 models and averaging? Uh-oh... our results may end up skewed. Ah, but I'm using weighted ensembling and I watch out for outliers. Great... but stacking may still help!\n",
    "\n",
    "Think about this - you run a tonne of models and you get some results and you ensemble them. All you are doing is some sort of simple model then right? But what if you could run your results through an XGB? Or a linear regression model? Would that perhaps make your results better than just a simple average?\n",
    "\n",
    "This is stacked generalization. Take your predictions - run them through another model and POOF! an \"learned\" ensemble.If you say, have 3 models to start (let us say this is level 0). We make our predictions in level 0 and say feed these results to a XGB in the next level, Level 1. It takes out three predictions and gives us another set based on our previous ones. Perfect!\n",
    "\n",
    "Can we be even better? Sure! Lets make Level 0 have 15 different model predictions. Based on our input data though, it doesn't make sense to combine all the models in Level 1. We decide to build 3 models in Level 1 and feed some predictions from Level 0 to the level 1 models (I won't say which - and hey, why not the same predictions from Level 0 models into 2 Level 1 models?). So, Level 1 spits out three sets of predictions (one from each model). Now what? Stack another model in Level 2! Use predictions from Level 1 (and hey, maybe Level 0 if you want) and create a set of predictions once again.\n",
    "\n",
    "The trick is here, you can keep stacking more and more levels. Of course, this becomes ridiculous after too many levels. Don't forget too... it may take a long time to run a simple stacked generalization versus a few models and ensemble them via averaging.\n",
    "\n",
    "Time to play and learn!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
